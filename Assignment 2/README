README

Assignment A2: Find Approximate using binning and sampling technique

Requirement: Before running any scripts please verify if you have following runtime env's/libs installed.
	1) Java: v1.6+ preferable 
	2) Ant: for building projects and creating executable jars
	3) Hadoop 
	4) input file for map-reduce program (either locally or in HDFS)
	
Assignment Details:
	1) As per problem statement, the approximate median calculation using map-reduce with technique of binning and sampling.
	2) Performance comparisons (of running MR job on AWS EMR and in standalone mode) are put in a separate file 'performance-report.pdf'.
	3) In order to compile and build jars for programs, we have used ANT utility, there are build.xml files for each program-version separately, in their own directories.
	
Introduction:
    This Map Reduce Job implementation uses Binning technique along with sampling of inputs to find out the approximate median of given input data-set points. It is similar to "Median from Frequency Tables" approach, i.e., in a frequency table, the observations are already arranged in an ascending order. We can obtain the median by looking for the value in the middle position. 
    
    Whereas in our problem, we have to implement the mapper which basically sorts the input values into their appropriate bins/buckets, then sort those buckets using secondary sort and then find the middle value of this total data-set i.e., median. 

    One important thing to notice here is "Bins" don't store the actual values but they store the counts of values in them (in that bin). That way a lot of computations and sorting and finding is avoided. At
    
    Reducer, we get the list of bin counts sorted by bin-numbers and we then calculate the total number of records in all bins (by adding up their individual counts) and find middle element index. 
    
    In order to find the actual median value - we start from first bin - add up their counts till we reach a bin(s) which contain the median value. At this point the median value is bin-value (i.e. bin-size * bin-number).

How does it work:
    Mapper takes the input - decides whether to process/sample it based on sampling rate, then emits the key-value pair which is "Catetgory-BinNumber" and "BinNumber-BinCount" both as composite key- value class objects. We have written a grouping comparator which groups the records at reducer/combiner based on Category value only. We have implemented the partitioner class which partitions the output records at map-task, based on Category value only, so that all the records(key- value pairs) from same category should go to single Reducer. We have also implemented Combiner which helps in reducing / compressing the list of key-values sent to Reducer, increasing the performance. Combiner basically combines (adds up) the counts from same category and same BinNumber. At Reducer, we get the list of bin counts sorted by bin-numbers and we then calculate the total number of records in all bins 

Improvements:
    As the bin-size increases the approximation error (in finding the accurate median value) also increases, so rather than taking the start-value of bin-range as the median answer, we can take the middle value of bin-range. 
    
How to execute/run programs: 	
	1) There are five separate directories, one for each version of program: 
			a. MedianPurchaseV1 - improved version from Assignment 1, 
			b. MedianPurchaseV2 - improved version from Assignment 1, 
			c. MedianPurchaseV3 - improved version from Assignment 1, 
			c. MedianPurchaseV4Old- improved version from Assignment 1, 
			d. MedianPurchaseV2New- =====> new implementation using BINs and Sample Rate
	
	2) Each Directory contains - 
		a) source directory (src), 
		b) lib directory which contains compile-time libraries, 
		c) build.xml (for building project/program with ANT script), 
		d) generate-jar.sh (just compiles and builds the jar for program with proper MANIFEST file mentioning main-class name), 
		e) run-program-only.sh (runs the program from already existing jar with given inputs) and 
		f) run-all.sh (Does both compile code, generates jar and runs the program with given inputs).
		
	3) Examples - how to execute above scripts.
		a) generate-jar.sh (NOTE: you might need to add execute permission to make this script executable)
		>> ./generate-jar.sh

		b) run-program.sh
		(NOTE: you might need to add execute permission to make this script executable)
		(NOTE: While running programs V2, V3, V4-old, V4-New on hadoop, make sure your files are put in HDFS.)
		>> ./run-program.sh input/purchases4.txt output

		c) run.sh 
		(NOTE: you might need to add execute permission to make this script executable)
		(NOTE: While running programs V2, V3, V4-old, V4-New on hadoop, make sure your files are put in HDFS.)
		>> ./run.sh input/purchases4.txt output 

		d) If you want to run the generated jar all by yourself using hadoop command, you can do that using 
		>> hadoop jar MedianPurchaseV2.jar input/purchases4.txt output
		
		or 
		
		>> java -jar MedianPurchaseV1.jar ../purchases4.txt output.txt
		Or with JVM parameters like
		>> java -Xms1400m -Xmx2000m -XX:ParallelGCThreads=4 -jar MedianPurchaseV1.jar ../purchases4.txt output.txt

		4) How to run new version 4 - using command line arguments:
		>> hadoop jar MedianPurchaseV4New.jar <input_path> <output_path> <bin_size> <sample_rate>
		>> hadoop jar MedianPurchaseV4New.jar input/purchases4.txt output 1 100
		
Important Note:
1) For V1 program, input file should be on local and not on HDFS (like for v2, v3, v4-old, V4-New).
2) For V2, V3, V4-old, V4-New - input file should be in HDFS (except when running in standalone mode).
3) Make sure hadoop is properly installed and configured before you run any of above programs.
